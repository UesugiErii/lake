## 参考及推荐阅读

[决策树算法原理(上)](https://www.cnblogs.com/pinard/p/6050306.html)

[决策树算法原理(下)](https://www.cnblogs.com/pinard/p/6053344.html)

[scikit-learn决策树算法类库使用小结](https://www.cnblogs.com/pinard/p/6056319.html)

前两个已经被浓缩为本文内容了

## ID3

使用 信息增益(4.2.1) 来选择最优属性

### 缺点

1. 无法处理连续值

2. 信息增益准则对可取数目较多的属性有所偏好

3. 没考虑缺失值

4. 没考虑过拟合

## C4.5

为了解决ID3的4个缺点进行了改进

1. 将连续的特征离散化(4.4.1)

2. 用增益率替代信息增益准则(4.2.2)

3. 缺失值处理(4.4.2)

4. 引入了正则化系数进行初步的剪枝

### 缺点

1. 剪枝方法有优化的空间

2. 生成的是多叉树, 在计算机中二叉树模型会比多叉树运算效率高

3. 只能用于分类, 不能用于回归

4. 使用了熵模型，里面有大量的耗时的对数运算,如果是连续值还有大量的排序运算

## CART

scikit-learn使用了优化版的CART算法作为其决策树算法的实现

### 最优特征选择方法

为了解决C4.5的缺点4, CART分类树算法使用基尼系数(4.2.3)来代替信息增益比, 计算更简单

为了解决C4.5的缺点2, CART分类树算法每次仅仅对某个特征的值进行二分, 从而建立的是一个二叉树

### 对于连续和离散特征处理的改进

#### 连续特征

选择基尼系数最小的点作为该连续特征的二元离散分类点。比如取到的基尼系数最小的点为at,则小于at的值为类别1，大于at的值为类别2

连续属性可多次使用: 如果当前节点为连续属性，则该属性后面还可以参与子节点的产生选择过程 

#### 离散特征

ID3或者C4.5，如果某个特征A被选取建立决策树节点，如果它有A1,A2,A3三种类别，我们会在决策树上一下建立一个三叉的节点。这样导致决策树是多叉树

但是CART分类树使用的方法不同，他采用的是不停的二分，还是这个例子，CART分类树会考虑把A分成{A1}和{A2,A3}, {A2}和{A1,A3}, {A3}和{A1,A2}三种情况，找到基尼系数最小的组合，比如{A2}和{A1,A3},然后建立二叉树节点，一个节点是A2对应的样本，另一个节点是{A1,A3}对应的节点。同时，由于这次没有把特征A的取值完全分开，后面我们还有机会在子节点继续选择到特征A来划分A1和A3

这和ID3或者C4.5不同，在ID3或者C4.5的一棵子树中，离散特征只会参与一次节点的建立

### 缺失值

缺失值的处理方法同C4.5

### CART分类树建立算法

#### 训练

1) 对于当前节点的数据集为D，如果样本个数小于阈值或者没有特征，则返回决策子树，当前节点停止递归。

2) 计算样本集D的基尼系数，如果基尼系数小于阈值，则返回决策树子树，当前节点停止递归。

3) 计算当前节点现有的各个特征的各个特征值对数据集D的基尼系数，对于离散值和连续值的处理方法和基尼系数的计算见第二节。缺失值的处理方法和上篇的C4.5算法里描述的相同。

4) 在计算出来的各个特征的各个特征值对数据集D的基尼系数中，选择基尼系数最小的特征A和对应的特征值a。根据这个最优特征和最优特征值，把数据集划分成两部分D1和D2，同时建立当前节点的左右节点，做节点的数据集D为D1，右节点的数据集D为D2.

5) 对左右的子节点递归的调用1-4步，生成决策树。

#### 测试

假如测试集里的样本A落到了某个叶子节点，而节点里有多个训练样本。则对于A的类别预测采用的是这个叶子节点里概率最大的类别

### CART回归树建立算法

CART回归树和CART分类树的建立和预测的区别主要有下面两点

#### 连续值的处理方法不同

CART回归树的度量目标是，对于任意划分特征A，对应的任意划分点s两边划分成的数据集D1和D2，求出使D1和D2各自集合的均方差最小，同时D1和D2的均方差之和最小所对应的特征和特征值划分点。表达式为：

[\underbrace{min}_{A,s}\Bigg[\underbrace{min}_{c_1}\sum\limits_{x_i \in D_1(A,s)}(y_i - c_1)^2 + \underbrace{min}_{c_2}\sum\limits_{x_i \in D_2(A,s)}(y_i - c_2)^2\Bigg]](https://latex.91maths.com/s/?JTVDdW5kZXJicmFjZSU3Qm1pbiU3RF8lN0JBJTJDcyU3RCU1Q0JpZ2clNUIlNUN1bmRlcmJyYWNlJTdCbWluJTdEXyU3QmNfMSU3RCU1Q3N1bSU1Q2xpbWl0c18lN0J4X2klMjAlNUNpbiUyMERfMShBJTJDcyklN0QoeV9pJTIwLSUyMGNfMSklNUUyJTIwJTJCJTIwJTVDdW5kZXJicmFjZSU3Qm1pbiU3RF8lN0JjXzIlN0QlNUNzdW0lNUNsaW1pdHNfJTdCeF9pJTIwJTVDaW4lMjBEXzIoQSUyQ3MpJTdEKHlfaSUyMC0lMjBjXzIpJTVFMiU1Q0JpZ2clNUQ=)

其中，c1为D1数据集的样本输出均值，c2为D2数据集的样本输出均值

### 预测的不同

它采用的是用最终叶子的均值或者中位数来预测输出结果

### 剪枝

太长了, 见[原文](https://www.cnblogs.com/pinard/p/6053344.html)

书本对应章节为4.3

### 缺点

1. 大多数时候，分类决策不应该是由某一个特征决定的，而是应该由一组特征决定的。这样决策得到的决策树更加准确, 这种决策树叫做多变量决策树(multi-variate decision tree)(4.5), 代表算法有OC1

2. 如果样本发生一点点的改动，就会导致树结构的剧烈改变。这个可以通过集成学习里面的随机森林之类的方法解决

## 决策树优缺点

### 优点

1）简单直观，生成的决策树很直观。

2）基本不需要预处理，不需要提前归一化，处理缺失值。

3）使用决策树预测的代价是O(log2m)。 m为样本数。

4）既可以处理离散值也可以处理连续值。很多算法只是专注于离散值或者连续值。

5）可以处理多维度输出的分类问题。

6）相比于神经网络之类的黑盒分类模型，决策树在逻辑上可以得到很好的解释

7）可以交叉验证的剪枝来选择模型，从而提高泛化能力。

8） 对于异常点的容错能力好，健壮性高。

### 缺点

1）决策树算法非常容易过拟合，导致泛化能力不强。可以通过设置节点最少样本数量和限制决策树深度来改进。

2）决策树会因为样本发生一点点的改动，就会导致树结构的剧烈改变。这个可以通过集成学习之类的方法解决。

3）寻找最优的决策树是一个NP难的问题，我们一般是通过启发式方法，容易陷入局部最优。可以通过集成学习之类的方法来改善。

4）有些比较复杂的关系，决策树很难学习，比如异或。这个就没有办法了，一般这种关系可以换神经网络分类方法来解决。

5）如果某些特征的样本比例过大，生成决策树容易偏向于这些特征。这个可以通过调节样本权重来改善。
