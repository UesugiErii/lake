## 原型聚类(9.4)

### K-Means算法(9.4.1)

k-平均聚类的目的是：把n个点划分到k个聚类中，使得每个点都属于离他最近的均值（此即聚类中心）对应的聚类，以之作为聚类的标准。这个问题将归结为一个把数据空间划分为Voronoi cells的问题

![](./k-means/k-means.jpg)

### 学习向量量化(LVQ)(9.4.2)

与 k 均值算法类似,"学习向量量化" (Learning Vector Quantization,简称 LVQ) 也是试图找到一组原型向量来刻画聚类结构, 但与一般聚类算法不同的是, LVQ 假设数据样本带有类别标记 ,学习过程利用样本的这些监督信息来辅助聚类

可看作通过聚类来形成类别"子类"结构,每个子类对应一个聚类簇

**就是在同一类别标记中的样本里再聚类**

![](./LVQ/LVQ.jpg)

![](./LVQ/example.jpg)

### 高斯混合聚类算法(9.4.3)

![](./GMM/generate.jpg)

Σαi = 1

假设样本的生成过程由高斯混合分布给出: 首先, 根据 α1, α2, ..., αk 定义的先验分布选择高斯混合成分 ,其中 αi 为选择第 i 个混合成分的概率;然后,根据被选择的混合成分的概率密度函数进行采样, 从而生成相应的样本.

![](./GMM/GMM.jpg)

## 密度聚类(9.5)

密度聚类亦称"基于密度的聚类" (density-based clustering) ,此类算法假设聚类结构能通过样本分布的紧密程度确定.通常情形下,密度聚类算法从样本密度的角度来考察样本之间的可连接性,并基于可连接样本不断扩展聚类簇以获得最终的聚类结果.

### DBSCAN(9.5)

DBSCAN 是一种著名的密度聚类算法, 它基于一组"邻域"(neigh-borhood) 参数 (ε, MinPts) 来刻画样本分布的紧密程度

![](./DBSCAN/DBSCAN.jpg)

其中N(x)表示x某领域内的样本, 反斜杠表示去除

## 层次聚类(9.6)

### AGNES(9.6)

最初将每个对象看成一个簇，然后将这些簇根据某种规则被一步步合并，就这样不断合并直到达到预设的簇类个数

![](./AGNES/AGNES.jpg)

图中有个错误, 第5行的 j 应该从 i+1 开始

### BIRCH

适合于数据量大，类别数K也比较多的情况, 运行速度很快，只需要单遍扫描数据集就能进行聚类

但是如果数据特征的维度非常大，比如大于20，则BIRCH不太适合，此时Mini Batch K-Means的表现较好

用层次方法来聚类和规约数据, 利用了一个树结构来帮助我们快速的聚类

[BIRCH聚类算法原理](https://www.cnblogs.com/pinard/p/6179132.html)

[用scikit-learn学习BIRCH聚类](https://www.cnblogs.com/pinard/p/6200579.html)

## Others

### 谱聚类

对数据分布的适应性更强，聚类效果也很优秀，同时聚类的计算量也小很多，更加难能可贵的是实现起来也不复杂。在处理实际的聚类问题时，个人认为谱聚类是应该首先考虑的几种算法之一

它的主要思想是把所有的数据看做空间中的点，这些点之间可以用边连接起来。距离较远的两个点之间的边权重值较低，而距离较近的两个点之间的边权重值较高，通过对所有数据点组成的图进行切图，让切图后不同的子图间边权重和尽可能的低，而子图内的边权重和尽可能的高

[谱聚类（spectral clustering）原理总结](https://www.cnblogs.com/pinard/p/6221564.html)

[用scikit-learn学习谱聚类](https://www.cnblogs.com/pinard/p/6235920.html)
