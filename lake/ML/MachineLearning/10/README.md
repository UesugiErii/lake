## 原理, 动机

### KNN

相邻的样本越相似, 用k个邻居的信息来预测

### MDS算法

若要求原始空间中样本之间的距离在低维空间中得以保持, 即得到"多维缩放" (Mult iple Dimensional Scaling,简称 MDS)

MDS采用欧式距离

### PCA

可以从两个角度考虑

1. 最小化重构误差

2. 最大化降维后样本点方差

### KPCA

对于输入空间中的矩阵 X ，我们先用一个非线性映射把 X 中的所有样本映射到一个高维甚至是无穷维的空间(称为特征空间，Feature space)，(使其线性可分)，然后在这个高维空间进行PCA降维

### Isomap

等度量映射(Isometric Mapping,简称 Isomap)[Tenenbaum et a l., 2000]的基本出发点,是认为低维流型嵌入到高维空间之后,直接在高维空间中计算直线距离具有误导性, 因为高维空间中的直线距离在低维嵌入流形上是不可达的.

所以不同与MDS使用欧式距离, Isomap则使用测地线距离

测地线距离则通过将局部看作欧式空间计算与k近邻点距离, 再用Dijkstra或Floyd算法计算任意两点之间距离

### LLE

与 Isomap 试图保持近邻样本之间的距离不同,局部线性嵌入 (Locally Linear Embedding,简称LLE) [Ro weis and Saul , 2000] 试图保持邻域内样本之间的线性关系

如果在高维空间中有 $X_i = W_{ij}*X_j 十 W_{ik}*X_k+ W_{il}*X_l$, LLE希望这个关系在低维空间中得以保持.

在[这篇文章](https://cloud.tencent.com/developer/article/1184584 )中还提到了很多扩展性方法, 如HLLE, MLLE, LTSA

### 度量学习(NCA)

在机器学习中, 对高维数据进行降维的主要目的是希望找一个合适的低维空间,在此空间中进行学习能比原始空间性能更好.事实上,每个空间对应了在样本属性上定义的一个距离度量,而寻找合适的空间,实质上就是在寻找一
个合适的距离度量.那么,为何不直接尝试"学习"出一个合适的距离度量呢?这就是度量学习 (metric learning) 的基本动机.

本质上就是学习出一个合适的距离度量, 使得使用此距离度量能获得更好的性能

