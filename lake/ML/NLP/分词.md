## jieba

[https://github.com/fxsjy/jieba](https://github.com/fxsjy/jieba)

## MiNLP

[https://github.com/XiaoMi/MiNLP/tree/main/minlp-tokenizer](https://github.com/XiaoMi/MiNLP/tree/main/minlp-tokenizer)

MiNLP-Tokenizer是小米AI实验室NLP团队自研的中文分词工具，基于深度学习序列标注模型实现，在公开测试集上取得了SOTA效果